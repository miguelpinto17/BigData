import pandas as pd
from pyspark.sql import SparkSession
import time
import psutil
import os
from pathlib import Path
import matplotlib.pyplot as plt

# For√ßar execu√ß√£o relativa ao local do script
os.chdir(os.path.dirname(os.path.abspath(__file__)))

# ========== Configura√ß√µes ==========
DATA_DIR = Path("../../data/county_partitioned_states")
OUTPUT_DIR = Path("../../results")
OUTPUT_FILE = OUTPUT_DIR / "benchmark_county_results.csv"
CHART_TIME = OUTPUT_DIR / "time_analysis_county.png"
CHART_MEM = OUTPUT_DIR / "memory_analysis_county.png"

# Criar diret√≥rio de resultados se n√£o existir
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ========= Fun√ß√µes de monitoriza√ß√£o =========
def memory_usage_mb():
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024  # em MB

def measure_python(df):
    start_mem = memory_usage_mb()
    start_time = time.perf_counter()

    result = df.groupby("State").count()

    end_time = time.perf_counter()
    end_mem = memory_usage_mb()

    return round(end_time - start_time, 4), round(end_mem - start_mem, 2)

def measure_pyspark(spark, df):
    start_mem = memory_usage_mb()
    start_time = time.perf_counter()

    result = df.groupBy("State").count()
    result.collect()

    end_time = time.perf_counter()
    end_mem = memory_usage_mb()

    return round(end_time - start_time, 4), round(end_mem - start_mem, 2)

# ========= Iniciar Spark =========
spark = SparkSession.builder \
    .appName("BenchmarkPythonVsPySpark") \
    .master("local[*]") \
    .getOrCreate()

# ========= Carregar dados =========
# Pandas
dfs = [pd.read_csv(file) for file in DATA_DIR.glob("*.csv")]
df_pandas = pd.concat(dfs, ignore_index=True)

# PySpark
df_spark = spark.read.option("header", True).csv(str(DATA_DIR / "*.csv"))

# ========= Executar benchmarks =========
print("\nüîç Executando benchmarks...")

time_py, mem_py = measure_python(df_pandas)
time_sp, mem_sp = measure_pyspark(spark, df_spark)

print(f"[Python] ‚è± {time_py}s | üíæ {mem_py}MB")
print(f"[PySpark] ‚è± {time_sp}s | üíæ {mem_sp}MB")

# ========= Salvar resultados =========
with open(OUTPUT_FILE, "w") as f:
    f.write("engine,time_sec,memory_mb\n")
    f.write(f"python,{time_py},{mem_py}\n")
    f.write(f"pyspark,{time_sp},{mem_sp}\n")

# ========= Fechar sess√£o Spark =========
spark.stop()

print("\nüìä A gerar gr√°ficos...")

# ========= Gerar Gr√°ficos =========
df = pd.read_csv(OUTPUT_FILE)

# Tempo
plt.figure(figsize=(8, 5))
plt.bar(df["engine"], df["time_sec"], color=["blue", "orange"])
plt.title("Tempo de Execu√ß√£o - Python vs PySpark")
plt.ylabel("Tempo (segundos)")
plt.tight_layout()
plt.savefig(CHART_TIME)
plt.close()

# Mem√≥ria
plt.figure(figsize=(8, 5))
plt.bar(df["engine"], df["memory_mb"], color=["blue", "orange"])
plt.title("Uso de Mem√≥ria - Python vs PySpark")
plt.ylabel("Mem√≥ria (MB)")
plt.tight_layout()
plt.savefig(CHART_MEM)
plt.close()

print("‚úÖ Gr√°ficos gerados em:")
print(f"   üìÅ {CHART_TIME}")
print(f"   üìÅ {CHART_MEM}")
print(f"üìÑ CSV: {OUTPUT_FILE}")
